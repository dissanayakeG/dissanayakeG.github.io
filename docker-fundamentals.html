<!DOCTYPE html>

<html lang="en">
	<head>
		<meta charset="UTF-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0" />
		<title>Document</title>
		<link
			rel="stylesheet"
			href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css"
		/>
		<link rel="stylesheet" href="./util/styles.css" />
		<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
		<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-json.min.js"></script>
		<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
	</head>

	<body>
		<div id="nav-icon" class="nav-icon">
			<div class="nav-icon-row"></div>
			<div class="nav-icon-row"></div>
			<div class="nav-icon-row"></div>
			<div class="nav-icon-row"></div>
		</div>

		<div id="nav-container" class="nav-container"></div>

		<section class="content">
			<br />
			<h1 id="basics-list-pull-run-and-delete">
				üê≥ Basics (List, Pull, Run and Delete)
			</h1>
			<br />
			<p>
				<strong>References</strong>
				<a
					href="https://docs.docker.com/get-started/introduction/whats-next/"
					>https://docs.docker.com/get-started/introduction/whats-next/</a
				>
				<a
					href="https://www.youtube.com/watch?v=b0HMimUb4f0&ab_channel=mCoding"
					>https://www.youtube.com/watch?v=b0HMimUb4f0&amp;ab_channel=mCoding</a
				>
			</p>
			<pre><code class="language-bash">docker image ls #list down all the images
docker ps #list down the running containers
docker ps -a #list down all the containers

#Pull Images
docker pull python #Will take around 1GB
docker pull python:3.11-slim #will takw around 140mb
docker pull python:3.11-alpine #will takw around 50mb

#slim images - usually Debial-based Linux
#alpine images - Alpine Linux

#Stop Docker container
docker stop &lt;container-name&gt; 

#Delete Docker Container/s
docker container prune #Remove all stopped containers
docker run -p 5000:80 -d --rm nginx # automatically delete when the container is stopped using --rm
docker rm -f &lt;container-name&gt; # Force remove a container (running or stopped)

#Delete a Docker image
docker rmi &lt;image_name&gt;:&lt;tag&gt;
docker rmi &lt;image_id&gt;
docker rmi -f &lt;image_name&gt;:&lt;tag&gt;
</code></pre>
			<br />
			<h2 id="start-and-run-new-container-using-remote-image">
				Start and run new container using remote Image
			</h2>
			<br />
			<pre><code class="language-bash">docker run --name=base-container -ti ubuntu
</code></pre>
			<ul>
				<li>
					-t: Allocates a pseudo-TTY (terminal) | terminal features
					like history and auto complete
				</li>
				<li>
					-i: Keeps STDIN open (interactive mode) | interactive
					terminal session
				</li>
			</ul>
			<br />
			<h2 id="build-custome-images-with-namestags">
				Build Custome Images with names/tags
			</h2>
			<br />
			<pre><code class="language-bash">#The most basic¬†`docker build`¬†command, build with no name/tag
#The final `.` in the command provides the path or URL to the build context. At this location,
#the builder will find the Dockerfile and other referenced files.

docker build .

#build with a name/tag
docker build -t my-username/my-image .

#add another tag for existing image
docker image tag my-username/my-image another-username/another-image:v1
</code></pre>
			<br />
			<h1 id="create-and-run-containers-using-images">
				Create and run containers using Images
			</h1>
			<br />
			<pre><code class="language-bash">#This builds a Docker image from a Dockerfile in the current directory (.), and tags it as mysite.
#You now have a custom Docker image called mysite on your machine.
docker build -t mysite .

#This creates and runs a container from the image mysite, gives it a name, and maps ports so you can access it from your browser.
docker run --name my-container-using-mysite -p 3000:80 mysite

docker run -p 3000:80 -d nginx

#-p : for port mapping
#-d : for detached mode | Run the container in the background
</code></pre>
			<br />
			<h2 id="port-mapping-with-nginx">Port mapping with NginX</h2>
			<br />
			<pre><code class="language-bash">docker run nginx #will pull if its not already pulled

#We can see this is running in the docker desktop, but if we visit localhost in the browser it shows an error
#This is because by default containers are isolated form the host machine.
#So, we have to publish the port, so host can see it

docker run -p HOST_PORT:CONTAINER_PORT nginx
docker run -p 80:80 nginx

#If you omit the HOST_PORT the container‚Äôs port 80 onto an ephemeral port on the host: and you can find it by
docker ps
#now we can see the localhost is working on the browser

docker run -P nginx #publish all of the exposed ports configured by the image
</code></pre>
			<br />
			<h2 id="debugging">Debugging</h2>
			<br />
			<pre><code class="language-bash">docker exec -it &lt;name or GUID&gt; /bin/bash

#-i : interactive terminal session
#-t : terminal features like history and auto complete

docker logs &lt;container-name&gt; or GUID #Get logs
</code></pre>
			<br />
			<h2 id="publishing">Publishing</h2>
			<br />
			<pre><code class="language-bash">docker push my-username/my-image
</code></pre>
			<br />
			<h1 id="dockerfile">Dockerfile</h1>
			<br />
			<pre><code class="language-bash">FROM node:20-alpine #define your base image

WORKDIR /app # This will specify where future commands will run and the directory files will be copied inside the container image.

COPY package.json yarn.lock ./ # Read below note

COPY . . #Copy all of the files from your project on your machine into the container image

RUN yarn install --production

EXPOSE 3000

CMD [&quot;node&quot;, &quot;./src/index.js&quot;] # start command
</code></pre>
			<p>
				<strong>Note:</strong> For Node-based applications, dependencies
				are defined in the package.json file. You&#39;ll want to
				reinstall the dependencies if that file changes, but use cached
				dependencies if the file is <strong>unchanged</strong>. So,
				start by copying only that file first, then install the
				dependencies, and finally copy everything else. Then, you only
				need to recreate the yarn dependencies if there was a change to
				the package.json file.
			</p>
			<p>
				Great start! Here&#39;s an improved and polished version of your
				Docker short note, with grammar corrections, clearer phrasing,
				and some extra helpful information added:
			</p>
			<br />
			<h1 id="overriding-container-defaults">
				Overriding container defaults
			</h1>
			<br />
			<br />
			<h2 id="overriding-the-network-ports">
				Overriding the network ports
			</h2>
			<br />
			<pre><code class="language-bash">#You can use the -p option in docker run to map container ports to host ports, allowing you to run the multiple instances of the container without any conflict.
docker run -d -p HOST_PORT:CONTAINER_PORT postgres
</code></pre>
			<br />
			<h2 id="setting-environment-variables">
				Setting environment variables
			</h2>
			<br />
			<pre><code class="language-bash">docker run -e foo=bar postgres env

#The .env file acts as a convenient way to set environment variables for your Docker containers without cluttering your command line #with numerous -e flags. To use a .env file, you can pass --env-file option with the docker run command.

docker run --env-file .env postgres env
</code></pre>
			<br />
			<h2 id="restricting-the-container-to-consume-the-resources">
				Restricting the container to consume the resources
			</h2>
			<br />
			<pre><code class="language-bash">#You can use the --memory and --cpus flags with the docker run command to restrict how much CPU and memory a container can use
docker run -e POSTGRES_PASSWORD=secret --memory=&quot;512m&quot; --cpus=&quot;0.5&quot; postgres

#You can use the docker stats command to monitor the real-time resource usage of running containers. 
</code></pre>
			<br />
			<h2 id="run-postgres-container-in-a-controlled-network">
				Run Postgres container in a controlled network
			</h2>
			<br />
			<pre><code class="language-bash">#By default, containers automatically connect to a special network called a bridge network when you run them. This bridge network acts like a virtual bridge, allowing containers on the same host to communicate with each other while keeping them isolated from the outside world and other hosts.

#You create a custom network by passing --network flag with the docker run command. All containers without a --network flag are attached to the default bridge network.

docker network create mynetwork
docker network ls

#Connect Postgres to the custom network by using the following command:
docker run -d -e POSTGRES_PASSWORD=secret -p 5434:5432 --network mynetwork postgres
</code></pre>
			<br />
			<h1 id="persisting-container-data">Persisting container data</h1>
			<br />
			<ul>
				<li>
					For example, if you restart a database container, you might
					not want to start with an empty database. So, how do you
					persist files?
				</li>
			</ul>
			<br />
			<h2 id="container-volumes">Container volumes</h2>
			<br />
			<ul>
				<li>
					Volumes are a storage mechanism that provide the ability to
					persist data beyond the lifecycle of an individual
					container. Think of it like providing a shortcut or symlink
					from inside the container to outside the container.
				</li>
			</ul>
			<pre><code class="language-bash">docker volume create log-data # create a volume named log-data
docker run -d -p 80:80 -v log-data:/logs docker/welcome-to-docker # the volume will be mounted (or attached) into the container at /logs

# Mounts a named volume called log-data to the container‚Äôs /logs directory.

# Any files written by the container to /logs will be stored in the log-data volume on your host, which persists even if the container is removed.
docker volume ls
docker volume inspect log-data
</code></pre>
			<ul>
				<li>
					If the volume log-data doesn&#39;t exist, Docker will
					automatically create it for you.
				</li>
				<li>
					When the container runs, all files it writes into the /logs
					folder will be saved in this volume, outside of the
					container. If you delete the container and start a new
					container using the same volume, the files will still be
					there.
				</li>
				<li>
					You can attach the same volume to multiple containers to
					share files between containers.
				</li>
				<li>
					Volumes have their own lifecycle beyond that of containers
					and can grow quite large depending on the type of data and
					applications you‚Äôre using.
				</li>
			</ul>
			<pre><code class="language-bash">docker volume ls # list all volumes
docker volume rm &lt;volume-name-or-id&gt; #  remove a volume (only works when the volume is not attached to any containers)
docker volume prune # remove all unused (unattached) volumes
</code></pre>
			<br />
			<h1 id="sharing-local-files-with-containers">
				Sharing local files with containers
			</h1>
			<br />
			<ul>
				<li>
					<strong>Containers are isolated</strong>, meaning they don‚Äôt
					access the host‚Äôs filesystem by default.
				</li>
				<li>
					To <strong>persist data</strong> or
					<strong>share files</strong> between host and container,
					Docker provides:
					<ul>
						<li>
							<strong>Volumes</strong>: Managed by Docker, ideal
							for <strong>persistent storage</strong>. Survive
							container restarts.
						</li>
						<li>
							<strong>Bind mounts</strong>: Link specific host
							directories/files to the container. Great for
							<strong>development</strong> and real-time updates.
						</li>
					</ul>
				</li>
			</ul>
			<br />
			<h3 id="code-vcode-vs-code--mountcode">
				üîπ <code>-v</code> vs <code>--mount</code>
			</h3>
			<br />
			<ul>
				<li>
					<code>-v /host/path:/container/path</code>:
					<ul>
						<li>Simple and auto-creates missing host dirs.</li>
						<li>Good for quick tasks.</li>
					</ul>
				</li>
				<li>
					<code
						>--mount
						type=bind,source=/host/path,target=/container/path</code
					>:
					<ul>
						<li>More readable and feature-rich.</li>
						<li>Fails if the host path doesn&#39;t exist.</li>
						<li>
							<strong>Recommended by Docker</strong> for better
							control.
						</li>
					</ul>
				</li>
			</ul>
			<p>
				Use <strong>volumes</strong> for persistent container data, and
				<strong>bind mounts</strong> to share real-time code or config
				files during development.
			</p>
			<pre><code class="language-bash">docker run -v /HOST/PATH:/CONTAINER/PATH -it nginx
docker run --mount type=bind,source=/HOST/PATH,target=/CONTAINER/PATH,readonly nginx
</code></pre>
			<br />
			<h3 id="docker-bind-mount-permissions-amp-performance-summary">
				Docker Bind Mount Permissions &amp; Performance Summary
			</h3>
			<br />
			<ul>
				<li>
					<p>
						<strong>Bind mounts</strong> allow containers to access
						and share files from the host system.
					</p>
				</li>
				<li>
					<p>
						Use access flags with the mount to control permissions:
					</p>
					<ul>
						<li>
							<code>:ro</code> ‚Üí <strong>Read-only</strong>:
							container can read but
							<strong>not modify/delete</strong> host files.
						</li>
						<li>
							<code>:rw</code> ‚Üí <strong>Read-write</strong>:
							container can
							<strong>read, modify, or delete</strong> host files.
						</li>
						<li>
							Example:
							<pre><code class="language-bash">docker run -v /host/dir:/container/dir:rw nginx
</code></pre>
						</li>
					</ul>
				</li>
				<li>
					<p>
						<strong>Best practice</strong>: Use <code>:ro</code> for
						config files to prevent accidental modification.
					</p>
				</li>
			</ul>
			<br />
			<h3 id="synchronized-file-share">Synchronized File Share</h3>
			<br />
			<ul>
				<li>
					For large or frequently accessed codebases, bind mounts can
					be slow.
				</li>
				<li>
					<strong>Synchronized file shares</strong> enhance
					performance using <strong>filesystem caching</strong>,
					especially in VM-based Docker setups (like Docker Desktop).
				</li>
				<li>
					They ensure faster and more efficient file access between
					host and container in development environments.
				</li>
			</ul>
			<pre><code class="language-bash">mkdir public_html
cd public_html
touch index.html #add some html content in the file
docker run -d --name my_site -p 8080:80 -v .:/usr/local/apache2/htdocs/ httpd:2.4
OR
docker run -d --name my_site -p 8080:80 --mount type=bind,source=./,target=/usr/local/apache2/htdocs/ httpd:2.4
# Now go to the docker desktop
# Open the container
# Goto the Files tab
# Search in /usr/local/apache2/htdocs/
</code></pre>
			<br />
			<h1 id="caching">Caching</h1>
			<br />
			<ul>
				<li>
					Docker uses a <strong>layered file system</strong>: each
					instruction in a <code>Dockerfile</code> creates a new
					layer.
				</li>
				<li>
					When a layer changes,
					<strong>all layers after it are invalidated</strong> and
					must be rebuilt.
				</li>
				<li>
					Most instructions are <strong>cached by default</strong> to
					improve build speed.
				</li>
				<li>
					Using <code>RUN</code> commands to delete files doesn&#39;t
					remove them from earlier layers; it just
					<strong>creates a new layer</strong>.
					<blockquote>
						<p>
							üîê <strong>Tip</strong>: Never store sensitive
							information in Docker images (e.g., API keys,
							passwords), as layers can be inspected.
						</p>
					</blockquote>
				</li>
			</ul>
			<pre><code class="language-bash">docker build --no-cache .  # Build image without using any cached layers
</code></pre>
			<br />
			<h1 id="multi-container-applications">
				Multi-container applications
			</h1>
			<br />
			<ul>
				<li>
					Used to <strong>separate build environment</strong> from the
					final runtime environment.
				</li>
				<li>Helps reduce image size and keep the final image clean.</li>
				<li>
					You can <strong>target a specific stage</strong> when
					building:
				</li>
			</ul>
			<pre><code class="language-bash">docker build -t mysite-frontend --target runner .
</code></pre>
			<br />
			<h2 id="docker-compose">Docker Compose</h2>
			<br />
			<ul>
				<li>
					Managing multiple containers (frontend, backend, DB)
					individually is complex.
				</li>
				<li>
					<strong>Docker Compose</strong> simplifies orchestration of
					multi-container applications.
				</li>
				<li>
					Define all services in a single
					<code>docker-compose.yml</code> file.
				</li>
			</ul>
			<br />
			<h3 id="example-codedocker-composeymlcode">
				Example <code>docker-compose.yml</code>
			</h3>
			<br />
			<pre><code class="language-yaml">version: &quot;3.8&quot;

services:
  backend:
    image: mysite-backend
    container_name: mysite-backend
    pull_policy: never  # Prevents accidentally pulling from Docker Hub
    build:
      context: ./backend
      dockerfile: Dockerfile
      target: runner  # Use specific stage from Dockerfile
    ports:
      - &quot;8000:8000&quot;

  frontend:
    image: mysite-frontend
    container_name: mysite-frontend
    pull_policy: never
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - &quot;80:80&quot;
</code></pre>
			<br />
			<h3 id="commands">Commands</h3>
			<br />
			<pre><code class="language-bash">docker compose build       # Build all services
docker compose up          # Start all services
docker compose down        # Stop and remove containers
</code></pre>
			<ul>
				<li>
					<code>pull_policy: never</code> helps avoid pulling images
					with the same name from Docker Hub.
				</li>
				<li>
					Docker Compose allows you to run
					<strong>the full stack with a single command</strong> ‚Äî
					great for development and testing!
				</li>
			</ul>
		</section>
		<script src="./util/createMenuBar.js"></script>
	</body>
</html>
